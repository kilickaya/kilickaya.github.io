<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-59337882-2"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59337882-2');
  </script>

  <title>Mert Kilickaya</title>
  <meta name="author" content="Mert Kilickaya">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar2.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mert Kilickaya</name>
              </p>
              <p>
                I am a Computer Vision researcher currently working as a Post-doc at <a href="https://joaquinvanschoren.github.io/home/#lab" target="_blank">Eindhoven University of Technology</a>. I got my PhD at <a href="https://ivi.fnwi.uva.nl/quva/" target="_blank">University of Amsterdam</a> advised by <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en" target="_blank">Professor Arnold Smeulders </a></a>. 
              </p>

              <p style="text-align:center">
                <a href="mailto:kilickayamert@gmail.com">Email</a> &nbsp/&nbsp
                <a href="resume/resume.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/mertkilickaya_" target="_blank">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mert-kilickaya/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/kilickaya" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/avatar2.jpg" width="180" style="border-radius:50%" class="profile-image">
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: Our survey on <a href="https://arxiv.org/pdf/2302.00353.pdf", target="_blank">label-efficient incremental learning</a> is online</li>                 
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2023: Our preprint on <a href="https://arxiv.org/pdf/2301.11417.pdf", target="_blank">self-incremental learning</a> is online</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: I completed my Ph.D. Find my thesis <a href="resume/thesis.pdf" target="_blank">here</a> <a href="resume/defense.pdf" target="_blank">(slides)</a>  </li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2021: Our work on <a href="https://arxiv.org/pdf/2112.00492.pdf", target="_blank">interaction detection</a> is accepted to <a href="https://www.bmvc2021-virtualconference.com/", target="_blank">BMVC-21</a></li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

      <div class="experience-section">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TUE.png" alt="PontTuset" width="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Learn Lab, Eindhoven University of Technology, Netherlands</papertitle>
              <br>
              2022-2023
              <br>
              <p>
                Post-doc Researcher 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/QUVA.png" alt="PontTuset" width="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>QUvA Deep Vision Lab, University of Amsterdam, Netherlands</papertitle>
              <br>
              2017-2022
              <br>
              <p>
                PhD Researcher 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Huawei.jpg" alt="PontTuset" width="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Huawei Visual Search Lab, Helsinki, Finland</papertitle>
              <br>
              2021 March - 2022 January
              <br>
              <p>
                Research Scientist Intern 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Laval.png" alt="PontTuset" width="75" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Universit√© Laval Computer Vision Lab, Quebec, Canada</papertitle>
              <br>
              2015 March - 2015 October 
              <br>
              <p>
                Research Intern 
              </p>
            </td>
          </tr>

    </tbody>
  </table>
</div>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <p>
          My research goal is to reduce the human supervision need of (continual) visual learners.   
        </p>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/survey_cil.PNG" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Label-Efficient Incremental Learning: A Survey</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=Gsw2iUEAAAAJ&hl=en", target="_blank">Joost van de Weijer</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=CdpLhlgAAAAJ", target="_blank">Yuki Asano</a>              
              <br>
              Preprint 2023
              <br>
              <a href="https://arxiv.org/pdf/2302.00353.pdf", target="_blank">arXiv</a> / 
              <p></p>
              <p></p>
              <p>
                We survey the very recent efforts in scaling up incremental learners via semi-, few-shot and self-supervision.    
              </p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VINIL.PNG" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Are Labels Needed for Incremental Instance Learning?</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=HhDsD9UAAAAJ&hl=en", target="_blank">Joaquin Vanschoren</a>
              <br>
              Preprint 2023
              <br>
              <a href="https://arxiv.org/pdf/2301.11417.pdf", target="_blank">arXiv</a> / <a href="resume/slides_vinil.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce VINIL, an incremental instance learning model that is purely self-supervised.   
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BMVC21.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Human-Object Interaction Detection via Weak Supervision</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              BMVC 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.00492.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_bmvc21.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce Align-Former, a visual transformer-based detector that can localize human-object interactions with only image-level supervision. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/WACV21.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Structured Visual Search via Composition-aware Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              WACV 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kilickaya_Structured_Visual_Search_via_Composition-Aware_Learning_WACV_2021_paper.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_wacv21.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce Composition-aware Learning (CAL), that leverages the symmetries within input (search query) and output (feature) spaces for interaction search. 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPRW20.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Continual Learning of Object Instances</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=75hA5MQAAAAJ&hl=en" target="_blank">Kishan Parshotam</a>,
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>
              <br>
              CVPRW 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Parshotam_Continual_Learning_of_Object_Instances_CVPRW_2020_paper.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_cvprw_20.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce incremental learning of object instances applied to cars. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICPR20.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Self-Selective Context for Interaction Recognition</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=xKixWowAAAAJ&hl=en", target="_blank">Noureldien Hussein</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Stratis Gavves</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              ICPR 2020
              <br>
              <a href="https://arxiv.org/pdf/2010.08750.pdf", target="_blank">arXiv</a> /
              <p></p>
              <p></p>
              <p>
                We introduce Self-Selective Context to select discriminative contextual features for interaction recognition. 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EACL17.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Re-evaluating Automatic Metrics for Image Captioning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=-xA1_OAAAAAJ&hl=en" target="_blank">Aykut Erdem</a>,
              <a href="https://scholar.google.com/citations?user=gwLessAAAAAJ&hl=en" target="_blank">Nazli Ikizler-Cinbis</a>,
              <a href="https://scholar.google.com/citations?user=eALwl74AAAAJ&hl=en" target="_blank">Erkut Erdem</a>

              <br>
              EACL 2017
              <br>
              <a href="https://arxiv.org/pdf/1612.07600.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_eacl17.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We evaluate existing image caption evaluation metrics, and introduce Word-Mover Distance to account for the semantic similarity.
              </p>
            </td>
          </tr>

        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Patents</heading>
              </td>
            </tr>
 
          <tr>
            <td width="75%" valign="middle">
              <papertitle>Visual Image Search via Conversational Interaction (Con-VIS)</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=hAubt7EAAAAJ&hl=zh-CN", target="_blank">Baiqiang XIA</a>
              <br>
              US Patent, 2022, <a href="xxx", target="_blank">Link</a>
              <br>
               
              <p></p>
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <papertitle>Network for Interacted Object Localization</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              US Patent, 2022, <a href="https://patentimages.storage.googleapis.com/b2/d1/a3/08bd2d6929cd20/US20220414371A1.pdf", target="_blank">Link</a>
              <br>
               
              <p></p>
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <papertitle>Subject-Object Interaction Recognition Model</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Stratis Gavves</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              US Patent, 2022, <a href="https://patentimages.storage.googleapis.com/7a/68/a0/a15807e2e82632/US11481576.pdf", target="_blank">Link</a>
              <br>
               
              <p></p>
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="middle">
              <papertitle>Context-driven Learning of Human-object Interactions</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=xKixWowAAAAJ&hl=en", target="_blank">Noureldien Hussein</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Stratis Gavves</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              US Patent, 2021, <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=MBMjO0sAAAAJ&sortby=pubdate&citation_for_view=MBMjO0sAAAAJ:ufrVoPGSRksC", target="_blank">Link</a>               

              <br>
              <p></p>
              <p></p>
            </td>
          </tr>

        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Supervision</heading>
                       <p>
            I am grateful to work with this list of amazing researchers!                       
              </p>
              
                 <p>
                <ul>
               <li>Ran Piao (MSc, 2023-)</li>                 
               <li>Elif Ceren Gok (PhD, 2023-)</li>                 
              <li><a href="https://scholar.google.com/citations?user=75hA5MQAAAAJ&hl=en">Kishan Parshotam</a> (MSc, 2020-21)</li>
              <li><a href="https://krishnatarun.github.io/">Tarun Krishna</a> (MSc,2019-20)</li>                 

              </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
              

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Presentations</heading>
              
                     <p>
            I like studying and presenting papers from other researchers. Big congrats to the authors for their awesome work!                      
              </p>
              
              <p>
                <ul>
              <li><a href="/resume/meta.pdf">Meta Learning General Purpose Learning Algorithms with Transformers</a></li>                 
              <li><a href="/resume/beit.pdf">BEiT: BERT Pre-training of Image Transformers</a></li>
              <li><a href="/resume/vip.pdf">Visual Parser (ViP): Representing Part-Whole Relations with Transformers</a></li>              
              <li><a href="/resume/clip.pdf">Learning Transferable Visual Models From Natural Language Supervision (CLIP)</a></li>
              <li><a href="/resume/iclr22.pdf">ICLR-22 Potpourri</a></li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
