<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-59337882-2"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-59337882-2');
  </script>

  <title>Mert Kilickaya</title>
  <meta name="author" content="Mert Kilickaya">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar2.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mert Kilickaya</name>
              </p>
              <p>
                I am a Computer Vision researcher. I currently work as a Post-doc at <a href="https://joaquinvanschoren.github.io/home/#lab" target="_blank">Learning to Learn Lab, Eindhoven University of Technology</a>. I got my PhD at <a href="https://ivi.fnwi.uva.nl/quva/" target="_blank">QUvA Lab, University of Amsterdam</a> advised by <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en" target="_blank">Professor Arnold Smeulders </a></a>. 
              </p>

              <p style="text-align:center">
                <a href="mailto:kilickayamert@gmail.com">Email</a> &nbsp/&nbsp
                <a href="resume/resume.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/mertkilickaya_" target="_blank">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mert-kilickaya/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/kilickaya" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/avatar2.jpg" width="180" style="border-radius:50%" class="profile-image">
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: I completed my Ph.D. Find my thesis <a href="resume/thesis.pdf" target="_blank">here</a> <a href="resume/defense.pdf" target="_blank">(slides)</a>  </li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2021: Our work on <a href="https://arxiv.org/pdf/2112.00492.pdf", target="_blank">human-object interaction detection</a> is accepted to <a href="https://www.bmvc2021-virtualconference.com/", target="_blank">BMVC-21</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: Started my research internship at <a href="https://www.huawei.com/en/" target="_blank">Huawei Finland</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2020: Our work on <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kilickaya_Structured_Visual_Search_via_Composition-Aware_Learning_WACV_2021_paper.pdf", target="_blank">visual interaction search</a> is accepted to <a href="https://wacv2021.thecvf.com/home", target="_blank">WACV-21</a></li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TUE.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Learn Lab, Eindhoven University of Technology, Eindhoven, Netherlands</papertitle>
              <br>
              2022-2023
              <br>
              <p>
                Post-doc Researcher 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/QUVA.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>QUvA Deep Vision Lab, University of Amsterdam, Amsterdam, Netherlands</papertitle>
              <br>
              2017-2022
              <br>
              <p>
                PhD Researcher 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Huawei.jpg" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Huawei Visual Search Lab, Helsinki, Finland</papertitle>
              <br>
              2021 March - 2022 January
              <br>
              <p>
                Research Scientist Intern 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Laval.png" alt="PontTuset" width="150" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Universit√© Laval Computer Vision Lab, Quebec, Canada</papertitle>
              <br>
              2015 March - 2015 October 
              <br>
              <p>
                Research Intern 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Hacettepe.png" alt="PontTuset" width="150" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Hacettepe University Computer Vision Lab, Ankara, Turkey</papertitle>
              <br>
              2014 - 2017 
              <br>
              <p>
                Graduate Research Assistant 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Bytesnap.png" alt="PontTuset" width="150" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ByteSnap, Birmingham, UK</papertitle>
              <br>
              2013 March - 2013 October 
              <br>
              <p>
                Software Engineering Intern 
              </p>
            </td>
          </tr>

    </tbody>
  </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <p>
          My research goal is to reduce the human supervision need of (continual) visual learners.   
        </p>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BMVC21.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Human-Object Interaction Detection via Weak Supervision</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              BMVC 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.00492.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_bmvc21.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce Align-Former, a visual transformer-based detector that can localize human-object interactions with only image-level supervision. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/WACV21.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Structured Visual Search via Composition-aware Learning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              WACV 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kilickaya_Structured_Visual_Search_via_Composition-Aware_Learning_WACV_2021_paper.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_wacv2121.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We introduce Composition-aware Learning (CAL), that leverages the symmetries within input (search query) and output (feature) spaces for interaction search. 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPRW20.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Continual Learning of Object Instances</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=75hA5MQAAAAJ&hl=en" target="_blank">Kishan Parshotam</a>,
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>
              <br>
              CVPRW 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Parshotam_Continual_Learning_of_Object_Instances_CVPRW_2020_paper.pdf", target="_blank">arXiv</a> /
              <p></p>
              <p></p>
              <p>
                We introduce incremental learning of object instances applied to cars. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICPR20.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Self-Selective Context for Interaction Recognition</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=xKixWowAAAAJ&hl=en", target="_blank">Noureldien Hussein</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Stratis Gavves</a>,
              <a href="https://scholar.google.nl/citations?user=aa5Ou7gAAAAJ&hl=en", target="_blank">Arnold Smeulders</a>
              <br>
              ICPR 2020
              <br>
              <a href="https://arxiv.org/pdf/2010.08750.pdf", target="_blank">arXiv</a> /
              <p></p>
              <p></p>
              <p>
                We introduce Self-Selective Context to select discriminative contextual features for interaction recognition. 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EACL17.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Re-evaluating Automatic Metrics for Image Captioning</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MBMjO0sAAAAJ&hl=en&oi=ao" target="_blank">Mert Kilickaya</a>,
              <a href="https://scholar.google.com/citations?user=-xA1_OAAAAAJ&hl=en" target="_blank">Aykut Erdem</a>,
              <a href="https://scholar.google.com/citations?user=gwLessAAAAAJ&hl=en" target="_blank">Nazli Ikizler-Cinbis</a>,
              <a href="https://scholar.google.com/citations?user=eALwl74AAAAJ&hl=en" target="_blank">Erkut Erdem</a>

              <br>
              EACL 2017
              <br>
              <a href="https://arxiv.org/pdf/1612.07600.pdf", target="_blank">arXiv</a> /  <a href="resume/slides_eacl17.pdf", target="_blank">Slides</a>
              <p></p>
              <p></p>
              <p>
                We evaluate existing image caption evaluation metrics, and introduce Word-Mover Distance to account for the semantic similarity.
              </p>
            </td>
          </tr>

        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Presentations</heading>
              
                     <p>
            I like studying and presenting papers from other researchers. Big congrats to the authors for their awesome work!                      
              </p>
              
              <p>
                <ul>
              <li><a href="/resume/meta.pdf">Meta Learning General Purpose Learning Algorithms with Transformers</a></li>                 
              <li><a href="/resume/beit.pdf">BEiT: BERT Pre-training of Image Transformers</a></li>
              <li><a href="/resume/vip.pdf">Visual Parser (ViP): Representing Part-Whole Relations with Transformers</a></li>              
              <li><a href="/resume/clip.pdf">CLIP</a></li>
              <li><a href="/resume/iclr22.pdf">ICLR-22 Potpourri</a></li>
              </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
